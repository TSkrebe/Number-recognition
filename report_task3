KNN:
Precision: 0.341
[[47  4  1 22  0  6  9  7  0  4]
 [ 5 23 12  6  9 10 16  9  8  2]
 [ 0 21 20  3 10 10  3 16 10  7]
 [23  9  6 32  6  5  6  8  0  5]
 [ 1 10  4  1 52 13  1 13  3  2]
 [ 9  8 15 10 16 11  2 11 12  6]
 [ 5 21  6  4  1  4 50  8  1  0]
 [ 3 18 14 10 12 11  7 18  3  4]
 [ 1  9  8  2  6 12  1  2 34 25]
 [ 0  3  1  7  1  7  1  4 22 54]]

Full Gaussian:
Precision: 0.444
[[55  4  2 32  0  0  7  0  0  0]
 [ 0 33 38 10  6  0 11  0  1  1]
 [ 0 16 61  7  7  0  2  0  7  0]
 [18 12  9 53  0  0  4  0  0  4]
 [ 0  4 21  2 66  1  0  0  6  0]
 [ 8  3 21 25 19  0  1  0 18  5]
 [ 4 21  6  6  0  0 63  0  0  0]
 [ 2 16 42 15 13  0  6  0  3  3]
 [ 1  2 16  5 10  1  0  0 50 15]
 [ 2  2  3  8  0  2  0  0 20 63]]

LDA Gaussian:
Precision: 0.413
[[59  2  0 30  0  2  7  0  0  0]
 [ 1 17 15  8  6 24 17 10  1  1]
 [ 0  7 27  2  6 36  8  6  8  0]
 [24  4  0 43  0 18  2  4  1  4]
 [ 0  2  8  1 66 19  0  1  3  0]
 [ 7  3  9 20 18 19  2  1 18  3]
 [ 6 11  3  3  0  3 70  4  0  0]
 [ 4  9 21 10 13 23  8  6  4  2]
 [ 1  1  2  2 11 19  0  0 43 21]
 [ 3  1  0  5  0 10  0  0 18 63]]

4)
How do the shapes of the decision boundaries look for each method?  Compare these methods based on
the visualisation of these decision boundaries.
For  higher  marks,  try  further  analysis  and  give  decent  discussions,  e.g.   show  two  decision  boundary
plots  in  which  you  only  show  the  decision  boundaries  and  data  points  corresponding  to  two  classes  of
your choosing which are clearly separable, and one plot in which they are not (NB: each plot concerns a
di erent pair of classes).  Explain why some classes are harder to sepa

